# Projeto 6 - Multi-Agentes de IA, RAG, Roteamento, Guardrails, Observabilidade e Explicabilidade com LangGraph, LangSmith e Pydantic LogFire
# M√≥dulo da App

# Importa o m√≥dulo os para opera√ß√µes de sistema como verificar arquivos e diret√≥rios
import os

# Importa Streamlit para criar a interface web interativa
import streamlit as st

# Importa fun√ß√£o para carregar vari√°veis de ambiente de um arquivo .env
from dotenv import load_dotenv

# Importa TypedDict e Literal para tipagem do estado do grafo
from typing import TypedDict, Literal

# Importa cliente Groq para chamadas ao LLM Groq
from langchain_groq import ChatGroq

# Importa FAISS para buscar documentos via RAG
from langchain_community.vectorstores import FAISS

# Importa modelo de embeddings FastEmbed para transformar textos em vetores
from langchain_community.embeddings import FastEmbedEmbeddings

# Importa ferramenta de busca DuckDuckGo
from langchain_community.tools import DuckDuckGoSearchRun

# Importa estrutura de grafo de estados e constante de fim
from langgraph.graph import StateGraph, END

# LogFire e LangSmith para Observabilidade
import logfire
from langsmith import traceable
from langsmith import Client as LangSmithClient 

########## Configura√ß√£o de Vari√°veis de Ambiente ##########

# Ativa o paralelismo de tokeniza√ß√£o
os.environ['TOKENIZERS_PARALLELISM'] = 'True'

# Configura√ß√£o da p√°gina da web app
st.set_page_config(page_title="Data Science Academy", page_icon=":100:", layout="centered")

# Carrega vari√°veis de ambiente definidas no arquivo .env
load_dotenv()

# Chave API Groq
groq_api_key = os.getenv("GROQ_API_KEY")
if not groq_api_key:
    logfire.critical("GROQ_API_KEY n√£o definida no ambiente ou .env!") # Usando logfire
    st.error("GROQ_API_KEY n√£o definida no ambiente ou .env!")
    st.stop()

# Chaves API LangSmith/LangChain
langsmith_api_key = os.getenv("LANGSMITH_API_KEY")
langchain_api_key_env = os.getenv("LANGCHAIN_API_KEY")

# Token LogFire
LOGFIRE_API_KEY = os.getenv("LOGFIRE_API_KEY")

# Configura√ß√£o do Pydantic LogFire
try:
    logfire.configure() 
    print("DSA Log - Logfire configurado.") 
except Exception as e:
     print(f"DSA Log - Alerta: Falha ao configurar Logfire automaticamente: {e}")

# Agora verifica as chaves e tokens, usando st.warning para feedback na UI

if not langsmith_api_key or not langchain_api_key_env:
    st.warning("LANGSMITH_API_KEY e/ou LANGCHAIN_API_KEY n√£o definidas. O tracing do LangSmith pode n√£o funcionar completamente.")

if not LOGFIRE_API_KEY:
     st.warning("LOGFIRE_API_KEY n√£o definida. Logs para Pydantic LogFire Cloud n√£o funcionar√£o (a menos que outro exportador OTEL esteja configurado).")

# Verifique LANGCHAIN_TRACING_V2
if os.getenv("LANGCHAIN_TRACING_V2", "false").lower() != "true":
    st.warning("Vari√°vel de ambiente LANGCHAIN_TRACING_V2 n√£o est√° como 'true'. O tracing autom√°tico do LangGraph para o LangSmith PODE estar desativado.")

# Verifica LANGCHAIN_API_KEY especificamente para tracing
if not langchain_api_key_env:
     st.warning("Vari√°vel de ambiente LANGCHAIN_API_KEY n√£o definida. O tracing do LangGraph para LangSmith N√ÉO funcionar√°.")

# Caminho RAG
VECTORSTORE_PATH = "dsa_faiss_index"

########## Fun√ß√µes Auxiliares ##########

# O carregamento do LLM consome muitos recursos e deve ser cacheado
# Decorador do Streamlit para armazenar em cache o recurso de carregamento do LLM
@st.cache_resource
# Define a fun√ß√£o respons√°vel por carregar o LLM que fornecer√° a resposta final
def dsa_carrega_llm_resposta_final():
    
    # Imprime no console que o processo de carregamento do LLM Groq foi iniciado
    print("DSA Log - Carregando LLM Groq...")
    
    # Inicia bloco try para capturar e tratar eventuais exce√ß√µes
    try:
        
        # Cria uma inst√¢ncia do modelo Groq usando a chave de API, o nome do modelo e a temperatura definidos
        # Este √© um LLM mais robusto que ser√° usado para a resposta final ao usu√°rio
        llm = ChatGroq(api_key = groq_api_key, model = "llama3-70b-8192", temperature = 0.1) 
        
        # Registra no LogFire que o LLM Groq para resposta final foi carregado com sucesso
        logfire.info("LLM Groq (resposta final) carregado com sucesso.")
        
        # Retorna o objeto do LLM para uso em outras partes da aplica√ß√£o
        return llm
    
    # Captura qualquer exce√ß√£o lan√ßada durante o carregamento do LLM
    except Exception as e:
        
        # Registra no LogFire o erro ocorrido, incluindo traceback para auxiliar no diagn√≥stico
        logfire.error("Erro ao carregar LLM final", error = str(e), exc_info = True)
        
        # Exibe uma mensagem de erro na interface Streamlit informando o usu√°rio sobre o problema
        st.error(f"Erro ao carregar LLM: {e}")
        
        # Interrompe a execu√ß√£o da aplica√ß√£o devido ao erro cr√≠tico no carregamento do LLM
        st.stop()

# O carregamento do retriever RAG pode consumir muitos recursos e deve ser cacheado
# Decorador do Streamlit que armazena em cache o resultado desta fun√ß√£o
@st.cache_resource
# Define a fun√ß√£o respons√°vel por carregar o retriever utilizado no RAG
def dsa_carrega_retriever():
    
    # Registra no console que o processo de carregamento do retriever foi iniciado
    print("DSA Log - Carregando Retriever RAG...")
    
    # Verifica se o diret√≥rio ou arquivo do √≠ndice FAISS existe no caminho especificado
    if not os.path.exists(VECTORSTORE_PATH):
        
        # Registra um erro no LogFire informando que o √≠ndice FAISS n√£o foi encontrado
        logfire.error("√çndice FAISS n√£o encontrado", path = VECTORSTORE_PATH)
        
        # Exibe uma mensagem de erro na interface Streamlit instruindo o usu√°rio a executar o setup
        st.error(f"√çndice FAISS n√£o encontrado em '{VECTORSTORE_PATH}'. Execute 'dsa_p6_setup_rag.py'.")
        
        # Interrompe a execu√ß√£o da aplica√ß√£o devido √† aus√™ncia do √≠ndice FAISS
        st.stop()

    # Tenta carregar o retriever dentro de um bloco que captura exce√ß√µes
    try:
        
        # Cria o modelo de embeddings FastEmbed com o nome do modelo BAAI/bge-small-en-v1.5
        embedding_model = FastEmbedEmbeddings(model_name = "BAAI/bge-small-en-v1.5")
        
        # Carrega o √≠ndice FAISS local utilizando o modelo de embeddings criado
        vector_store = FAISS.load_local(VECTORSTORE_PATH, embedding_model, allow_dangerous_deserialization = True)
        
        # Converte o vector_store em um retriever, definindo o n√∫mero de resultados de busca como 5
        retriever = vector_store.as_retriever(search_kwargs = {'k': 5})
        
        # Registra no LogFire que o retriever foi carregado com sucesso, indicando o caminho utilizado
        logfire.info("Retriever RAG carregado com sucesso.", path = VECTORSTORE_PATH)
        
        # Retorna o objeto retriever para uso nas opera√ß√µes de RAG
        return retriever
    
    # Em caso de qualquer erro durante o processo de carregamento
    except Exception as e:
        
        # Registra o erro no LogFire, incluindo detalhes da exce√ß√£o e traceback
        logfire.error("Erro ao carregar Retriever RAG", path = VECTORSTORE_PATH, error = str(e), exc_info = True) 
        
        # Exibe uma mensagem de erro na interface Streamlit com a descri√ß√£o da exce√ß√£o
        st.error(f"Erro ao carregar Retriever RAG: {e}")
        
        # Interrompe a aplica√ß√£o devido ao erro cr√≠tico no carregamento do retriever
        st.stop()

########## Fun√ß√µes Para os N√≥s do Grafo no LangGraph ##########

# Define a classe de estado do grafo (Agente de IA)
class GraphState(TypedDict):
    query: str
    source_decision: Literal["RAG", "WEB", ""]
    rag_context: str | None
    web_results: str | None
    final_answer: str | None

# Fun√ß√£o para o n√≥ de roteamento
@traceable(run_type = "llm", name = "Node_RouteQuery") # LangSmith Decorator
def dsa_route_query_node(state: GraphState) -> dict:
    """
    Analisa a consulta e decide a fonte de dados (RAG ou WEB).
    Atualiza 'source_decision' no estado.
    """

    # Extrai a query do estado
    query = state["query"]

    # Logfire span para agrupar logs do n√≥
    span = logfire.span("Executando N√≥: Roteamento da Consulta", query = query)
    
    # Dentro do Span
    with span:

        # **PROMPT REFINADO COM EXEMPLOS (FEW-SHOT)**
        prompt = f"""Sua tarefa √© classificar a consulta de um usu√°rio para direcion√°-la √† melhor fonte de informa√ß√£o. As fontes s√£o:
        1.  **RAG**: Base de conhecimento interna com documentos de suporte t√©cnico, procedimentos espec√≠ficos, configura√ß√µes do nosso sistema, guias internos. Use RAG para perguntas sobre 'como fazer X em nosso sistema', 'qual a configura√ß√£o de Y', 'documenta√ß√£o interna sobre Z'.
        2.  **WEB**: Busca geral na internet para informa√ß√µes sobre software de terceiros (ex: Anaconda, Python, Excel), not√≠cias de tecnologia, erros gen√©ricos n√£o documentados internamente, informa√ß√µes muito recentes, ou qualquer coisa que n√£o seja espec√≠fica dos nossos documentos internos.

        Exemplos:
        - Consulta: "Como configuro o servidor de email interno?" -> Resposta: RAG
        - Consulta: "Qual a vers√£o mais recente do Streamlit?" -> Resposta: WEB
        - Consulta: "Qual o procedimento para resetar a senha do sistema ABC?" -> Resposta: RAG
        - Consulta: "Como instalo o interpretador Python no Windows 11" -> Resposta: WEB
        - Consulta: "como instalar o Anaconda Python" -> Resposta: WEB

        Agora, classifique a seguinte consulta:
        Consulta do Usu√°rio: '{query}'

        Com base na consulta, qual √© a fonte mais apropriada? Responda APENAS com a palavra 'RAG' ou a palavra 'WEB'."""

        # Bloco de execu√ß√£o
        try:

            # **CRIA LLM DEDICADO PARA ROTEAMENTO**
            router_llm = ChatGroq(api_key = groq_api_key,
                                  model = "llama3-8b-8192", # Modelo mais r√°pido para roteamento
                                  temperature = 0.0)        # Baixa temperatura para decis√£o

            # Executa o roteador 
            response = router_llm.invoke(prompt)

            # Extrai a decis√£o da resposta do LLM
            raw_decision = response.content

            # Limpeza do texto de resposta
            decision = raw_decision.strip().upper().replace("'", "").replace('"', '')

            # L√≥gica de decis√£o final
            if decision == "RAG":
                final_decision = "RAG"
            elif decision == "WEB":
                 final_decision = "WEB"
            else: # Fallback para WEB se a resposta n√£o for exatamente RAG ou WEB
                logfire.warn("Decis√£o inesperada do roteador, usando WEB como fallback.", raw_decision = raw_decision, query = query, decision_parsed = decision)
                final_decision = "WEB"

            logfire.info("Decis√£o de roteamento finalizada", raw_decision = raw_decision, final_decision = final_decision)

            return {"source_decision": final_decision}

        except Exception as e:
            logfire.error("Erro no n√≥ de roteamento, usando WEB como fallback.", query = query, error = str(e), exc_info = True)
            return {"source_decision": "WEB"}

# Fun√ß√£o para o n√≥ de retrieve do RAG
# Aplica o decorador de rastreamento do LangSmith, definindo tipo de execu√ß√£o e nome do n√≥
@traceable(run_type = "retriever", name = "Node_RetrieveRAG") # LangSmith Decorator
# Define a fun√ß√£o que recebe o estado do grafo e retorna um dicion√°rio com o contexto RAG
def dsa_retrieve_rag_node(state: GraphState) -> dict:
    
    # Extrai a consulta armazenada no estado do grafo
    query = state["query"]
    
    # Inicia um span no LogFire para rastrear a execu√ß√£o deste n√≥, incluindo a consulta
    span = logfire.span("Executando N√≥: Retrieve RAG", query = query)
    
    # Abre o contexto do span para agrupar todas as opera√ß√µes dentro deste n√≥
    with span:
        
        # Inicia um bloco try para capturar e tratar exce√ß√µes durante o retrieve
        try:
            
            # Carrega o retriever RAG a partir da fun√ß√£o cacheada
            local_retriever = dsa_carrega_retriever()
            
            # Executa a consulta usando o retriever carregado
            results = local_retriever.invoke(query)
            
            # Concatena o conte√∫do de cada documento retornado em uma √∫nica string de contexto
            context = "\n\n".join([doc.page_content for doc in results])

            # Verifica se n√£o foi encontrado nenhum conte√∫do relevante
            if not context:
                
                # Registra no LogFire que nenhum contexto RAG foi localizado
                logfire.info("Nenhum contexto RAG encontrado.")
                
                # Retorna mensagem informando que n√£o h√° documentos internos relevantes
                return {"rag_context": "N√£o foram encontrados documentos internos relevantes."}
            
            # Caso o contexto recuperado n√£o esteja vazio
            else:
                
                # Registra no LogFire que o contexto foi encontrado e informa o tamanho do texto
                logfire.info("Contexto RAG encontrado.", context_length = len(context))
                
                # Coment√°rio opcional para depura√ß√£o: registrar um trecho do contexto (cuidado com dados sens√≠veis)
                # logfire.debug("Trecho do contexto RAG:", context_snippet=context[:200])
                # Retorna o contexto completo recuperado
                return {"rag_context": context}
        
        # Captura qualquer exce√ß√£o ocorrida durante o processo de retrieve
        except Exception as e:
            
            # Registra o erro no LogFire, incluindo a consulta, mensagem e traceback
            logfire.error("Erro no n√≥ RAG", query = query, error = str(e), exc_info = True)
            
            # Retorna um dicion√°rio indicando erro ao buscar nos documentos internos
            return {"rag_context": f"Erro ao buscar nos documentos internos: {e}"}

# Fun√ß√£o para o n√≥ de busca na web
# Aplica o decorador de rastreamento do LangSmith, definindo tipo de execu√ß√£o e nome do n√≥
@traceable(run_type = "tool", name = "Node_SearchWeb") # LangSmith Decorator
# Define a fun√ß√£o que recebe o estado do grafo e retorna um dicion√°rio com os resultados da web
def dsa_search_web_node(state: GraphState) -> dict:
    
    # Extrai a consulta armazenada no estado do grafo
    query = state["query"]
    
    # Inicia um span no LogFire para rastrear a execu√ß√£o deste n√≥, incluindo a consulta
    span = logfire.span("Executando N√≥: Search Web", query = query)
    
    # Abre o contexto do span para agrupar todas as opera√ß√µes dentro deste n√≥
    with span:
        
        # Inicia um bloco try para capturar e tratar exce√ß√µes durante a busca na web
        try:
            
            # Cria a inst√¢ncia da ferramenta de busca DuckDuckGo
            web_search_tool = DuckDuckGoSearchRun()
            
            # Executa a consulta usando a ferramenta de busca
            results = web_search_tool.run(query)

            # Verifica se n√£o houve resultados retornados
            if not results:
                
                # Registra no LogFire que nenhum resultado foi encontrado
                logfire.info("Nenhum resultado da busca web.")
                
                # Retorna mensagem informando aus√™ncia de resultados relevantes
                return {"web_results": "N√£o foram encontrados resultados relevantes na web."}
            
            # Caso existam resultados
            else:
                
                # Registra no LogFire que resultados foram encontrados, incluindo a quantidade
                logfire.info("Resultados da busca web encontrados.", results_length = len(results))
                
                # Coment√°rio opcional para depura√ß√£o: registrar trecho dos resultados (cuidado com tamanho)
                # logfire.debug("Trecho resultados web:", results_snippet=results[:200])
                # Retorna os resultados completos da busca
                return {"web_results": results}
        
        # Captura qualquer exce√ß√£o ocorrida durante o processo de busca na web
        except Exception as e:
            
            # Registra o erro no LogFire com detalhes da exce√ß√£o e traceback
            logfire.error("Erro no n√≥ Web Search", query = query, error = str(e), exc_info = True)
            
            # Retorna um dicion√°rio indicando erro ao realizar a busca
            return {"web_results": f"Erro ao realizar busca na web: {e}"}

# Fun√ß√£o para gerar a resposta final para o usu√°rio
@traceable(run_type = "llm", name = "Node_GenerateAnswer") # LangSmith Decorator
def dsa_generate_answer_node(state: GraphState) -> dict:
    
    # Extrai a query do estado
    query = state["query"]

    # Span do LogFire
    span = logfire.span("Executando N√≥: Gera√ß√£o da Resposta", query = query)
    
    # Contexto do Span
    with span:

        # Atributos do estado
        rag_context = state.get("rag_context")
        web_results = state.get("web_results")
        context_provided = ""
        source_used = "Nenhuma"

        # Determina se contexto RAG e WEB est√£o presentes
        rag_useful = rag_context != "N√£o foram encontrados documentos internos relevantes."
        web_useful = web_results != "N√£o foram encontrados resultados relevantes na web."

        # Verifica a condi√ß√£o e define os atributos
        if rag_useful:
            context_provided = f"Contexto dos documentos internos:\n{rag_context}"
            source_used = "RAG"
            logfire.info("Usando contexto RAG para gerar resposta.")
        elif web_useful:
            context_provided = f"Resultados da busca na web:\n{web_results}"
            source_used = "WEB"
            logfire.info("Usando resultados da web para gerar resposta.")
        else:
            context_provided = "Nenhuma informa√ß√£o adicional encontrada nas fontes dispon√≠veis."
            logfire.info("Nenhum contexto √∫til encontrado para gerar resposta.")

        # Log da fonte utilizada
        logfire.info('Fonte(s) para gera√ß√£o', source_used = source_used, rag_context_present = rag_useful, web_results_present = web_useful)

        prompt = f"""Voc√™ √© um assistente de suporte t√©cnico prestativo e conciso. Responda √† pergunta do usu√°rio de forma clara, utilizando APENAS as informa√ß√µes fornecidas no contexto abaixo. Se o contexto n√£o for √∫til ou relevante para a pergunta, diga que n√£o encontrou informa√ß√µes espec√≠ficas sobre isso nas fontes dispon√≠veis. N√ÉO invente respostas.

        Consulta do Usu√°rio: {query}

        Contexto Fornecido:
        {context_provided}

        Resposta Concisa:"""

        # Bloco de execu√ß√£o
        try:

            # Executa a fun√ß√£o
            llm_resposta_final = dsa_carrega_llm_resposta_final()
            
            # Executa o LLM
            response = llm_resposta_final.invoke(prompt)

            # Extrai o conte√∫do da resposta
            final_answer = response.content

            # LogFire
            logfire.info("Resposta final gerada", source_used = source_used, answer_length = len(final_answer))
            
            return {"final_answer": final_answer}
       
        except Exception as e:
            logfire.error("Erro no n√≥ de gera√ß√£o da resposta", query = query, source_used = source_used, error = str(e), exc_info = True)
            return {"final_answer": f"Desculpe, ocorreu um erro t√©cnico ao tentar gerar a resposta final: {e}"}

# Fun√ß√£o para n√≥ de decis√£o da fonte (RAG ou WEB)
# N√£o precisa de @traceable pois √© l√≥gica simples, mas podemos logar
def dsa_decide_source_edge(state: GraphState) -> Literal["retrieve_rag_node", "search_web_node"]:
    decision = state["source_decision"]
    logfire.debug("Aresta Condicional: Decidindo pr√≥ximo n√≥", current_decision = decision) # LogFire n√≠vel debug

    if decision == "RAG":
        return "retrieve_rag_node"
    else: # Inclui "WEB" e qualquer fallback
        return "search_web_node"

########## Fun√ß√£o Para Compilar o Grafo e Definir Regra de Roteamento ##########

# Fun√ß√£o para compilar (criar) o grafo do LangGraph
@st.cache_resource # Cacheia o grafo compilado
def dsa_compile_graph():
    
    # Span
    span = logfire.span("Compilando o grafo LangGraph")
    
    # Contexto do Span
    with span:
        
        print("DSA Log - Compilando o grafo LangGraph...") # Mant√©m o print para feedback no console durante o cache
        
        try:
            
            # Cria os n√≥s
            graph_builder = StateGraph(GraphState)
            graph_builder.add_node("route_query_node", dsa_route_query_node)
            graph_builder.add_node("retrieve_rag_node", dsa_retrieve_rag_node)
            graph_builder.add_node("search_web_node", dsa_search_web_node)
            graph_builder.add_node("generate_answer_node", dsa_generate_answer_node)
            graph_builder.set_entry_point("route_query_node")

            # Cria a condi√ß√£o para o roteamento
            graph_builder.add_conditional_edges("route_query_node", dsa_decide_source_edge, {
                "retrieve_rag_node": "retrieve_rag_node",
                "search_web_node": "search_web_node",
            })

            # Adiciona as arestas sequenciais
            graph_builder.add_edge("retrieve_rag_node", "generate_answer_node")
            graph_builder.add_edge("search_web_node", "generate_answer_node")
            graph_builder.add_edge("generate_answer_node", END) # Termina o grafo ap√≥s a gera√ß√£o

            # Compila o grafo
            app = graph_builder.compile()
            print("DSA Log - Grafo compilado com sucesso!")
            
            logfire.info("Grafo LangGraph compilado com sucesso")
            
            return app
        
        except Exception as e:
            
            print(f"DSA Log - Erro GRAVE ao compilar o grafo: {e}") # Print para erro cr√≠tico
            
            logfire.critical("Erro ao compilar o grafo LangGraph", error = str(e), exc_info = True)
            
            # Levanta o erro para o Streamlit tratar (que j√° faz st.error e st.stop)
            raise e

########## Configura√ß√£o do Streamlit ##########

# T√≠tulos da app
st.title("Data Science Academy - Projeto 6")
st.title("ü§ñ Assistente de Suporte T√©cnico Interativo") 

# Inicializa o hist√≥rico de chat no session_state se n√£o existir
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ol√°! Como posso ajudar com seu suporte t√©cnico hoje?"}]

# Exibe mensagens do hist√≥rico a cada rerun
for message in st.session_state.messages:
    
    # Inicia um bloco de mensagem de chat no Streamlit usando o papel (role) da mensagem
    with st.chat_message(message["role"]):
        
        # Renderiza o conte√∫do da mensagem como markdown no chat
        st.markdown(message["content"])
        
        # Verifica se a mensagem foi gerada pelo assistente e inclui informa√ß√£o de fonte
        if message["role"] == "assistant" and "source" in message:
            
            # Exibe a fonte consultada como uma legenda abaixo da mensagem do assistente
            st.caption(f"Fonte consultada: {message['source']}")

# Carrega LLM, Retriever e compila o grafo (usando cache)
llm_final = dsa_carrega_llm_resposta_final()
retriever_rag = dsa_carrega_retriever()
compiled_app = dsa_compile_graph()

# Input do usu√°rio via chat_input
if user_query := st.chat_input("Sua pergunta sobre suporte t√©cnico..."):
    
    # Cria um span no LogFire para rastrear toda a requisi√ß√£o, armazenando a consulta do usu√°rio
    span_chat = logfire.span("Processando consulta do usu√°rio via Chat Interface", query = user_query) # Span para toda a requisi√ß√£o
    
    # Abre o contexto do span para agrupar as opera√ß√µes de processamento da consulta
    with span_chat:

        # Registra no LogFire que uma nova consulta foi recebida via chat_input
        logfire.info("Recebida nova consulta do chat input.")

        # Adiciona a mensagem do usu√°rio ao hist√≥rico de chat na sess√£o e a exibe
        st.session_state.messages.append({"role": "user", "content": user_query})
        
        # Exibe no chat a mensagem do usu√°rio com o papel "user"
        with st.chat_message("user"):
            st.markdown(user_query)

        # Inicia o bloco para exibir a resposta do assistente no chat
        with st.chat_message("assistant"):
            
            # Usa st.status para fornecer feedback de carregamento de forma detalhada
            with st.status("Pensando... üß†", expanded = False) as status:
                
                # Inicia o bloco try para capturar erros no processamento do agente
                try:
                    
                    # Exibe texto indicando an√°lise da pergunta e decis√£o sobre a fonte
                    st.write("Analisando sua pergunta e decidindo a melhor fonte...")
                    
                    # Prepara o dicion√°rio de inputs para invocar o grafo de agentes
                    inputs = {"query": user_query}

                    # Executa o grafo LangGraph compilado com os inputs fornecidos
                    final_state = compiled_app.invoke(inputs)

                    # Atualiza o status indicando qual fonte est√° sendo consultada
                    st.write(f"Consultando {final_state.get('source_decision', 'fonte desconhecida')}...") # Atualiza o status

                    # Extrai a resposta final do estado retornado pelo grafo
                    final_answer = final_state.get("final_answer", "N√£o foi poss√≠vel gerar uma resposta.")
                    
                    # Extrai a decis√£o da fonte usada
                    source = final_state.get('source_decision', 'N/A')

                    # Atualiza o status para completo, indicando que a resposta est√° pronta
                    status.update(label = "Resposta pronta!", state = "complete", expanded = False) # Atualiza status para completo

                # Em caso de erro na invoca√ß√£o do grafo, captura a exce√ß√£o
                except Exception as e:
                    
                    # Registra no LogFire o erro ocorrido, incluindo a consulta e o traceback
                    logfire.error("Erro ao invocar o grafo LangGraph principal a partir do Chat Interface", query = user_query, error = str(e), exc_info = True)
                    
                    # Exibe mensagem de erro na interface do usu√°rio com detalhes da exce√ß√£o
                    st.error(f"Ocorreu um erro inesperado. Detalhe: {e}")
                    
                    # Define a resposta exibida como mensagem de erro
                    final_answer = f"Desculpe, ocorreu um erro t√©cnico: {e}"
                    
                    # Define a fonte como "Erro" para registro
                    source = "Erro"
                    
                    # Atualiza o status indicando erro no processamento
                    status.update(label = "Erro no processamento", state = "error", expanded = True)

            # Exibe a resposta final gerada pelo assistente no chat
            st.markdown(final_answer)

            # Adiciona a resposta do assistente ao hist√≥rico de chat, incluindo a fonte consultada
            st.session_state.messages.append({
                "role": "assistant",
                "content": final_answer,
                "source": source # Armazena a fonte junto com a mensagem
            })

# Instru√ß√µes podem ir para um expander ou modal em vez da sidebar para um look mais limpo
with st.expander("‚ÑπÔ∏è Instru√ß√µes e Observa√ß√µes"):
    st.write("""
        - Fa√ßa perguntas espec√≠ficas sobre sua d√∫vida t√©cnica.
        - O sistema decidir√° automaticamente a melhor fonte (documentos internos ou web).
        - IA Generativa pode cometer erros. SEMPRE valide informa√ß√µes cr√≠ticas.
        - Logs e Traces s√£o enviados para **Pydantic LogFire** e **LangSmith**.
    """)
    if st.button("Suporte DSA"):
        st.write("D√∫vidas? Envie um e-mail para: suporte@datascienceacademy.com.br")

