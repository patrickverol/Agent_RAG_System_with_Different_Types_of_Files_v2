# Projeto 7 - Sistema Web de Suporte T√©cnico Automatizado com Agentic RAG e LLM Routing
# M√≥dulo de App com Agentic RAG e Roteamento

# Importa o m√≥dulo os para opera√ß√µes de sistema como verificar arquivos e diret√≥rios
import os

# Importa Streamlit para criar a interface web interativa
import streamlit as st

# Importa fun√ß√£o para carregar vari√°veis de ambiente de um arquivo .env
from dotenv import load_dotenv

# Importa TypedDict e Literal para tipagem do estado do grafo
from typing import TypedDict, Literal

# Importa classes de mensagem base e mensagem humana para LLM routing
from langchain_core.messages import BaseMessage, HumanMessage

# Importa cliente Groq para chamadas ao LLM Groq
from langchain_groq import ChatGroq

# Importa FAISS para buscar documentos via RAG
from langchain_community.vectorstores import FAISS

# Importa modelo de embeddings FastEmbed para transformar textos em vetores
from langchain_community.embeddings import FastEmbedEmbeddings

# Importa ferramenta de busca DuckDuckGo 
from langchain_community.tools import DuckDuckGoSearchRun

# Importa estrutura de grafo de estados e constante de fim
from langgraph.graph import StateGraph, END

# Carrega vari√°veis de ambiente definidas em .env
load_dotenv()

# Chave API Groq
groq_api_key = os.getenv("GROQ_API_KEY")
if not groq_api_key:
    st.error("GROQ_API_KEY n√£o definida no ambiente ou .env!")
    st.stop() 

# Caminho RAG 
VECTORSTORE_PATH = "dsa_faiss_index"

# Cache para o LLM (recurso pesado)
@st.cache_resource
def dsa_carrega_llm_resposta_final():
    
    print("Carregando LLM Groq...") 
    
    try:
        llm = ChatGroq(api_key = groq_api_key, model = "meta-llama/llama-4-maverick-17b-128e-instruct", temperature = 0.1)
        return llm
    except Exception as e:
        st.error(f"Erro ao carregar LLM: {e}")
        st.stop()

# Cache para o Retriever RAG (recurso pesado)
@st.cache_resource
def dsa_carrega_retriever():
    
    print("Carregando Retriever RAG...") 
    
    if not os.path.exists(VECTORSTORE_PATH):
        st.error(f"√çndice FAISS n√£o encontrado em '{VECTORSTORE_PATH}'. Execute 'dsa_p7_setup_rag.py'.")
        st.stop()
    
    try:
        embedding_model = FastEmbedEmbeddings(model_name = "BAAI/bge-small-en-v1.5")
        vector_store = FAISS.load_local(VECTORSTORE_PATH, embedding_model, allow_dangerous_deserialization = True)
        retriever = vector_store.as_retriever(search_kwargs = {'k': 5})
        return retriever
    except Exception as e:
        st.error(f"Erro ao carregar Retriever RAG: {e}")
        st.stop()

# Define a classe de estado do grafo (Agente de IA)
class GraphState(TypedDict):
    query: str
    source_decision: Literal["RAG", "WEB", ""]
    rag_context: str | None
    web_results: str | None
    final_answer: str | None

# Fun√ß√£o para o n√≥ de roteamento
def dsa_route_query_node(state: GraphState) -> dict:
    
    """
    Analisa a consulta e decide a fonte de dados (RAG ou WEB).
    Atualiza 'source_decision' no estado.
    """
    
    print("--- N√≥: Roteamento da Consulta ---")
    
    query = state["query"]

    # **PROMPT REFINADO COM EXEMPLOS (FEW-SHOT)**
    prompt = f"""Sua tarefa √© classificar a consulta de um usu√°rio para direcion√°-la √† melhor fonte de informa√ß√£o. As fontes s√£o:
    1.  **RAG**: Base de conhecimento interna com documentos de suporte t√©cnico, procedimentos espec√≠ficos, configura√ß√µes do nosso sistema, guias internos. Use RAG para perguntas sobre 'como fazer X em nosso sistema', 'qual a configura√ß√£o de Y', 'documenta√ß√£o interna sobre Z'.
    2.  **WEB**: Busca geral na internet para informa√ß√µes sobre software de terceiros (ex: Anaconda, Python, Excel), not√≠cias de tecnologia, erros gen√©ricos n√£o documentados internamente, informa√ß√µes muito recentes, ou qualquer coisa que n√£o seja espec√≠fica dos nossos documentos internos.

    Exemplos:
    - Consulta: "Como configuro o servidor de email interno?" -> Resposta: RAG
    - Consulta: "Qual a vers√£o mais recente do Streamlit?" -> Resposta: WEB
    - Consulta: "Qual o procedimento para resetar a senha do sistema ABC?" -> Resposta: RAG
    - Consulta: "Como instalo o interpretador Python no Windows 11" -> Resposta: WEB
    - Consulta: "como instalar o Anaconda Python" -> Resposta: WEB

    Agora, classifique a seguinte consulta:
    Consulta do Usu√°rio: '{query}'

    Com base na consulta, qual √© a fonte mais apropriada? Responda APENAS com a palavra 'RAG' ou a palavra 'WEB'."""

    try:
        # **CRIA LLM DEDICADO PARA ROTEAMENTO COM TEMPERATURA MAIS ALTA**
        # Aqui podemos usar um modelo mais simples, como um SLM
        router_llm = ChatGroq(api_key = groq_api_key,
                              model = "llama3-8b-8192",
                              temperature = 0.4)

        # Executa o roteador
        response = router_llm.invoke(prompt)

        # Extrai a resposta
        raw_decision = response.content 

        # DEBUG PRINT ESSENCIAL (Verificar no console!)
        print(f"DEBUG: Decis√£o do LLM (Roteador de Requisi√ß√µes): '{raw_decision}'")

        # Limpeza do texto de resposta para manter somente a palavra que nos interessa
        decision = raw_decision.strip().upper().replace("'", "").replace('"', '') 

        # L√≥gica de decis√£o final (se n√£o for RAG, assume WEB)
        if decision == "RAG":
            final_decision = "RAG"
        else:
            # Se n√£o for RAG, e tamb√©m n√£o for WEB, loga o valor mas vai para WEB
            if decision != "WEB":
                 print(f"  Decis√£o inv√°lida/inesperada do roteador: '{raw_decision}'. Usando WEB como fallback.") 
            final_decision = "WEB"

        print(f"  Decis√£o Final do Roteador: {final_decision}") 

        return {"source_decision": final_decision}

    except Exception as e:
        print(f"  Erro no n√≥ de roteamento: {e}") 
        print("  Usando WEB como fallback devido a erro.") 
        return {"source_decision": "WEB"}

# Fun√ß√£o para o n√≥ de retrieve do RAG
def dsa_retrieve_rag_node(state: GraphState) -> dict:
    
    query = state["query"]
    
    try:
        local_retriever = dsa_carrega_retriever() 
        results = local_retriever.invoke(query)
        context = "\n\n".join([doc.page_content for doc in results])
        if not context:
            print("  Nenhum contexto RAG encontrado.") 
            return {"rag_context": "N√£o foram encontrados documentos internos relevantes."}
        else:
            print(f"  Contexto RAG encontrado ({len(context)} chars).") 
            return {"rag_context": context}
    except Exception as e:
        print(f"  Erro no n√≥ RAG: {e}") 
        return {"rag_context": f"Erro ao buscar nos documentos internos: {e}"}

# Fun√ß√£o para o n√≥ de busca na web
def dsa_search_web_node(state: GraphState) -> dict:
    
    query = state["query"]
    
    try:
        web_search_tool = DuckDuckGoSearchRun()
        results = web_search_tool.run(query)
        if not results:
            print("  Nenhum resultado da busca web.") 
            return {"web_results": "N√£o foram encontrados resultados relevantes na web."}
        else:
            print(f"  Resultados da busca web encontrados ({len(results)} chars).") 
            return {"web_results": results}
    except Exception as e:
        print(f"  Erro no n√≥ Web Search: {e}") 
        return {"web_results": f"Erro ao realizar busca na web: {e}"}

# Fun√ß√£o para gerar a resposta final para o usu√°rio
def dsa_generate_answer_node(state: GraphState) -> dict:
    
    print("--- N√≥: Gera√ß√£o da Resposta ---") 
    
    query = state["query"]
    rag_context = state.get("rag_context")
    web_results = state.get("web_results")
    context_provided = ""
    source_used = "Nenhuma"

    if rag_context != "N√£o foram encontrados documentos internos relevantes.":
        context_provided = f"Contexto dos documentos internos:\n{rag_context}"
        source_used = "RAG"
        print("  Usando contexto RAG para gerar resposta.") 
    elif web_results != "N√£o foram encontrados resultados relevantes na web.":
        context_provided = f"Resultados da busca na web:\n{web_results}"
        source_used = "WEB"
        print("  Usando resultados da web para gerar resposta.") 
    else:
        context_provided = "Nenhuma informa√ß√£o adicional encontrada nas fontes dispon√≠veis."
        print("  Nenhum contexto √∫til encontrado para gerar resposta.") 

    prompt = f"""Voc√™ √© um assistente de suporte t√©cnico. Responda √† pergunta do usu√°rio de forma clara e concisa, utilizando APENAS as informa√ß√µes fornecidas no contexto abaixo, se houver.
    Consulta do Usu√°rio: {query}
    {context_provided}
    Resposta:"""

    try:
        llm_resposta_final = dsa_carrega_llm_resposta_final()
        response = llm_resposta_final.invoke(prompt)
        final_answer = response.content
        print(f"  Resposta gerada usando fonte: {source_used}") 
        return {"final_answer": final_answer}
    except Exception as e:
        print(f"  Erro no n√≥ de gera√ß√£o: {e}") 
        return {"final_answer": f"Desculpe, ocorreu un erro ao gerar a resposta: {e}"}

# Fun√ß√£o para n√≥ de decis√£o da fonte (RAG ou WEB)
def dsa_decide_source_edge(state: GraphState) -> Literal["retrieve_rag_node", "search_web_node"]:
    
    decision = state["source_decision"]
    
    print(f"--- Aresta Condicional: Decis√£o recebida = '{decision}' ---") 
    
    if decision == "RAG":
        print("  Aresta: Indo para RAG.") 
        return "retrieve_rag_node"
    else:
        print("  Aresta: Indo para WEB.") 
        return "search_web_node"

# Fun√ß√£o para compilar (criar) o grafo do LangGraph (ou seja, o Agente de IA)
@st.cache_resource
def dsa_compile_graph():
    
    print("Compilando o grafo LangGraph...") 
    
    # Cria os n√≥s
    graph_builder = StateGraph(GraphState)
    graph_builder.add_node("route_query_node", dsa_route_query_node)
    graph_builder.add_node("retrieve_rag_node", dsa_retrieve_rag_node)
    graph_builder.add_node("search_web_node", dsa_search_web_node)
    graph_builder.add_node("generate_answer_node", dsa_generate_answer_node)
    graph_builder.set_entry_point("route_query_node")
    
    # Cria a condi√ß√£o para o roteamento
    graph_builder.add_conditional_edges("route_query_node", dsa_decide_source_edge, {
        "retrieve_rag_node": "retrieve_rag_node",
        "search_web_node": "search_web_node",
    })
    
    # Adiciona as arestas
    graph_builder.add_edge("retrieve_rag_node", "generate_answer_node")
    graph_builder.add_edge("search_web_node", "generate_answer_node")
    graph_builder.add_edge("generate_answer_node", END)
    
    # Compila o grafo
    try:
        app = graph_builder.compile()
        print("Grafo compilado com sucesso!") 
        return app
    except Exception as e:
        st.error(f"Erro ao compilar o grafo: {e}")
        st.stop()

# Configurar p√°gina do Streamlit
st.set_page_config(page_title="Data Science Academy", page_icon=":100:", layout="centered")
st.title("Data Science Academy - Projeto 7")
st.title("ü§ñ Sistema Web de Suporte T√©cnico Automatizado com Agentic RAG e LLM Routing")
st.markdown("Fa√ßa sua pergunta sobre suporte t√©cnico. O sistema decidir√° se busca em documentos internos (RAG) ou na web.")

# Carrega LLM, Retriever e compila o grafo
llm = dsa_carrega_llm_resposta_final()
retriever = dsa_carrega_retriever()
app = dsa_compile_graph()

# Input do usu√°rio
user_query = st.text_input("Sua pergunta:", key = "query_input")

# Bot√£o de envio
if st.button("Buscar Resposta", key = "submit_button"):
    
    if user_query:
        
        with st.spinner("Processando sua consulta..."):
            
            try:
                inputs = {"query": user_query}

                # Executa o grafo (Agente de IA)
                final_state = app.invoke(inputs) 

                st.subheader("Resposta:")
                st.markdown(final_state.get("final_answer", "N√£o foi poss√≠vel gerar uma resposta."))
                st.info(f"Fonte utilizada (decidido pelo roteador): {final_state.get('source_decision', 'N/A')}")

            except Exception as e:
                st.error(f"Ocorreu um erro ao processar sua consulta: {e}")
    else:
        st.warning("Por favor, digite sua pergunta.")

# Barra lateral com instru√ß√µes de uso
st.sidebar.title("Instru√ß√µes")
st.sidebar.write("""
    - Digite perguntas espec√≠ficas sobre sua d√∫vida.
    - O sistema vai "raciocinar" e usar a base de dados do RAG para gerar respostas customizadas ou pesquisar na web.
    - Documentos, manuais e procedimentos complementares podem ser usados para aperfei√ßoar o sistema de RAG.
    - IA Generativa comete erros. SEMPRE valide as respostas.
""")

# Bot√£o de suporte na barra lateral que exibe mensagem ao clicar
if st.sidebar.button("Suporte"):
    st.sidebar.write("D√∫vidas? Envie um e-mail para: suporte@datascienceacademy.com.br")




    